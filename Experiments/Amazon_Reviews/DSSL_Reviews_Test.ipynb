{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: invalid argument)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "sys.path.insert(0, \"/home/cwhuang/DVTL/Model/\")\n",
    "import nnet as nn\n",
    "import criteria\tas er\n",
    "import util\n",
    "import DSSL\n",
    "\n",
    "import DataPackage as dp\n",
    "import AmazonReviewsFeaturePlot as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of minibatch at one epoch: 20, batch size source : 80, target : 80 \n",
      " validation size, S:400, T:400, test size, S:4465, T:3586\n",
      "... building the model\n",
      "Encoder is constructed with hidden layer number 1\n",
      "Classifier is constructed with hidden layer number 0\n",
      "Decoder is constructed with hidden layer number 1\n",
      "Encoder is constructed with hidden layer number 1\n",
      "Classifier is constructed with hidden layer number 0\n",
      "Decoder is constructed with hidden layer number 1\n",
      "Encoder is constructed with hidden layer number 1\n",
      "Classifier is constructed with hidden layer number 0\n",
      "Decoder is constructed with hidden layer number 1\n",
      "... training\n",
      "Initial, test accuracy: source domain :50.705487 %, target domain 50.390407 %\n",
      "epoch 1, minibatch 20/20, training loss 49.499572, validation loss 49.497730 \n",
      "     epoch 1, minibatch 20/20, test accuracy of best model: source domain :72.116461 %, target domain 66.648076 %\n",
      "epoch 2, minibatch 20/20, training loss 49.429883, validation loss 49.447560 \n",
      "     epoch 2, minibatch 20/20, test accuracy of best model: source domain :70.459127 %, target domain 67.819297 %\n",
      "epoch 3, minibatch 20/20, training loss 49.304629, validation loss 49.371680 \n",
      "     epoch 3, minibatch 20/20, test accuracy of best model: source domain :80.022396 %, target domain 76.882320 %\n",
      "epoch 4, minibatch 20/20, training loss 49.166822, validation loss 49.301286 \n",
      "     epoch 4, minibatch 20/20, test accuracy of best model: source domain :82.015677 %, target domain 79.531511 %\n",
      "epoch 5, minibatch 20/20, training loss 49.060515, validation loss 49.275436 \n",
      "     epoch 5, minibatch 20/20, test accuracy of best model: source domain :81.388578 %, target domain 79.838260 %\n",
      "epoch 6, minibatch 20/20, training loss 48.994510, validation loss 49.289265 \n",
      "epoch 7, minibatch 20/20, training loss 48.952092, validation loss 49.325274 \n",
      "epoch 8, minibatch 20/20, training loss 48.940494, validation loss 49.374187 \n",
      "epoch 9, minibatch 20/20, training loss 48.939472, validation loss 49.378568 \n",
      "epoch 10, minibatch 20/20, training loss 48.923703, validation loss 49.405587 \n",
      "epoch 11, minibatch 20/20, training loss 48.940582, validation loss 49.482508 \n",
      "epoch 12, minibatch 20/20, training loss 48.993441, validation loss 49.553533 \n",
      "epoch 13, minibatch 20/20, training loss 48.922860, validation loss 49.481803 \n",
      "epoch 14, minibatch 20/20, training loss 48.905930, validation loss 49.499695 \n",
      "epoch 15, minibatch 20/20, training loss 48.905790, validation loss 49.516172 \n",
      "epoch 16, minibatch 20/20, training loss 48.913209, validation loss 49.560053 \n",
      "epoch 17, minibatch 20/20, training loss 48.893267, validation loss 49.535344 \n",
      "epoch 18, minibatch 20/20, training loss 48.886361, validation loss 49.549901 \n",
      "epoch 19, minibatch 20/20, training loss 48.880345, validation loss 49.511081 \n",
      "epoch 20, minibatch 20/20, training loss 48.877088, validation loss 49.511086 \n",
      "epoch 21, minibatch 20/20, training loss 48.874948, validation loss 49.511079 \n",
      "epoch 22, minibatch 20/20, training loss 48.863025, validation loss 49.505724 \n",
      "epoch 23, minibatch 20/20, training loss 48.861659, validation loss 49.510917 \n",
      "epoch 24, minibatch 20/20, training loss 48.858390, validation loss 49.516430 \n",
      "epoch 25, minibatch 20/20, training loss 48.859342, validation loss 49.515068 \n",
      "epoch 26, minibatch 20/20, training loss 48.856533, validation loss 49.521615 \n",
      "epoch 27, minibatch 20/20, training loss 48.855105, validation loss 49.526095 \n",
      "epoch 28, minibatch 20/20, training loss 48.854956, validation loss 49.534925 \n",
      "epoch 29, minibatch 20/20, training loss 48.853399, validation loss 49.535678 \n",
      "epoch 30, minibatch 20/20, training loss 48.852541, validation loss 49.540187 \n",
      "epoch 31, minibatch 20/20, training loss 48.850474, validation loss 49.550028 \n",
      "epoch 32, minibatch 20/20, training loss 48.851269, validation loss 49.555223 \n",
      "epoch 33, minibatch 20/20, training loss 48.848986, validation loss 49.555732 \n",
      "epoch 34, minibatch 20/20, training loss 48.847712, validation loss 49.555528 \n",
      "epoch 35, minibatch 20/20, training loss 48.845429, validation loss 49.564695 \n",
      "epoch 36, minibatch 20/20, training loss 48.845196, validation loss 49.571934 \n",
      "epoch 37, minibatch 20/20, training loss 48.843438, validation loss 49.579532 \n",
      "epoch 38, minibatch 20/20, training loss 48.842358, validation loss 49.584233 \n",
      "epoch 39, minibatch 20/20, training loss 48.841600, validation loss 49.584792 \n",
      "epoch 40, minibatch 20/20, training loss 48.840875, validation loss 49.590229 \n",
      "epoch 41, minibatch 20/20, training loss 48.840185, validation loss 49.605554 \n",
      "epoch 42, minibatch 20/20, training loss 48.840014, validation loss 49.620687 \n",
      "epoch 43, minibatch 20/20, training loss 48.839228, validation loss 49.612532 \n",
      "epoch 44, minibatch 20/20, training loss 48.837744, validation loss 49.609339 \n",
      "epoch 45, minibatch 20/20, training loss 48.836556, validation loss 49.631084 \n",
      "epoch 46, minibatch 20/20, training loss 48.835582, validation loss 49.636152 \n",
      "epoch 47, minibatch 20/20, training loss 48.835949, validation loss 49.636158 \n",
      "epoch 48, minibatch 20/20, training loss 48.833882, validation loss 49.647205 \n",
      "epoch 49, minibatch 20/20, training loss 48.833393, validation loss 49.664259 \n",
      "epoch 50, minibatch 20/20, training loss 48.832857, validation loss 49.656903 \n",
      "epoch 51, minibatch 20/20, training loss 48.831241, validation loss 49.662139 \n",
      "epoch 52, minibatch 20/20, training loss 48.831291, validation loss 49.671849 \n",
      "epoch 53, minibatch 20/20, training loss 48.830098, validation loss 49.697675 \n",
      "epoch 54, minibatch 20/20, training loss 48.829523, validation loss 49.679076 \n",
      "epoch 55, minibatch 20/20, training loss 48.829588, validation loss 49.687924 \n",
      "epoch 56, minibatch 20/20, training loss 48.829783, validation loss 49.717100 \n",
      "epoch 57, minibatch 20/20, training loss 48.828505, validation loss 49.706987 \n",
      "epoch 58, minibatch 20/20, training loss 48.828203, validation loss 49.723089 \n",
      "epoch 59, minibatch 20/20, training loss 48.827827, validation loss 49.732788 \n",
      "epoch 60, minibatch 20/20, training loss 48.827560, validation loss 49.712096 \n",
      "epoch 61, minibatch 20/20, training loss 48.826631, validation loss 49.737854 \n",
      "epoch 62, minibatch 20/20, training loss 48.825875, validation loss 49.755324 \n",
      "epoch 63, minibatch 20/20, training loss 48.824343, validation loss 49.747145 \n",
      "epoch 64, minibatch 20/20, training loss 48.826572, validation loss 49.757341 \n",
      "epoch 65, minibatch 20/20, training loss 48.824891, validation loss 49.786220 \n",
      "epoch 66, minibatch 20/20, training loss 48.824739, validation loss 49.770284 \n",
      "epoch 67, minibatch 20/20, training loss 48.825147, validation loss 49.758434 \n",
      "epoch 68, minibatch 20/20, training loss 48.823012, validation loss 49.764633 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-251f8f364103>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cwhuang/DVLTL/DVLTL/DSSL.pyc\u001b[0m in \u001b[0;36mDSSL_training\u001b[1;34m(source_data, target_data, n_train_batches, n_epochs, struct, coef, description)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[0mminibatch_avg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;31m# iteration number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Model Construct'''\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    max_feature=2000\n",
    "    source_data, target_data = dp.datapackage(source='books', target='dvd', max_feature=max_feature, tfidf_setting='seperate')\n",
    "    \n",
    "    #source_data, target_data = DataDuplicate.DataDuplicate(source_data, target_data)\n",
    "    \n",
    "    ########################################################################\n",
    "    ###                        Coefficient Initial                       ###\n",
    "    ########################################################################        \n",
    "\n",
    "    x_dim = max_feature\n",
    "    y_dim = 2\n",
    "    z_dim = 50                #dimension of latent feature    \n",
    "    h_dim = 500             #dimension of hidden unit      \n",
    "    activation = T.nnet.sigmoid\n",
    "    lr = 0.005\n",
    "\n",
    "    struct = DSSL.DSSL_struct()        \n",
    "    struct.encoder.layer_dim = [x_dim, h_dim, z_dim]\n",
    "    struct.encoder.activation = [activation, activation]\n",
    "    struct.encoder.learning_rate = [lr, lr]\n",
    "    struct.encoder.decay = [1, 1]                \n",
    "    struct.classifier.layer_dim = [z_dim, y_dim]\n",
    "    struct.classifier.activation = [T.nnet.softmax]   \n",
    "    struct.classifier.learning_rate = [lr, lr]\n",
    "    struct.classifier.decay = [1, 1]\n",
    "    struct.decoder.layer_dim = [z_dim, h_dim, x_dim]\n",
    "    struct.decoder.activation = [activation, activation ]   \n",
    "    struct.decoder.learning_rate = [lr, lr]\n",
    "    struct.decoder.decay = [1, 1]    \n",
    "    \n",
    "    coef = DSSL.DSSL_coef(    \n",
    "        alpha = 1,\n",
    "        beta = 0.1,\n",
    "        D = 500,\n",
    "        optimize = 'Adam_update'        \n",
    "    )        \n",
    "    \n",
    "    isMMD = 'MMD_'\n",
    "    if coef.beta == 0:\n",
    "        isMMD = ''                \n",
    "    description = 'AmazonReviews_mf%i_DSSL_%s' % (max_feature, coef.optimize)\n",
    "        \n",
    "    features_model, test_model, trained_param = DSSL.DSSL_training(\n",
    "        source_data = source_data,\n",
    "        target_data = target_data,\n",
    "        n_train_batches = 20,\n",
    "        n_epochs=100,\n",
    "        struct = struct,\n",
    "        coef = coef,\n",
    "        description = description\n",
    "    )\n",
    "    \n",
    "    sample_n = 200\n",
    "    fp.features_plot(features_model, test_model, source_data, target_data, sample_n, description)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
